{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/26 19:56:49 WARN Utils: Your hostname, hp-computer resolves to a loopback address: 127.0.1.1; using 192.168.178.105 instead (on interface wlp0s20f3)\n",
      "24/02/26 19:56:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/26 19:56:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 Operations on Spark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loading the green dataset\n",
    "df_green = spark.read.parquet(\"../data/data/raw/green/*/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Converting this SQL query to RDD computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    data_trunc('hour', lpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(*) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2019-12-18 16:52:30|  2019-12-18 16:54:39|                 N|       1.0|         264|         264|            5.0|          0.0|        3.5|  0.5|    0.5|      0.01|         0.0|     null|                  0.3|        4.81|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:45:58|  2020-01-01 01:56:39|                 N|       5.0|          66|          65|            2.0|         1.28|       20.0|  0.0|    0.0|      4.06|         0.0|     null|                  0.3|       24.36|         1.0|      2.0|                 0.0|\n",
      "|       2| 2020-01-01 01:41:38|  2020-01-01 01:52:49|                 N|       1.0|         181|         228|            1.0|         2.47|       10.5|  0.5|    0.5|      3.54|         0.0|     null|                  0.3|       15.34|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 01:52:46|  2020-01-01 02:14:21|                 N|       1.0|         129|         263|            2.0|          6.3|       21.0| 3.25|    0.5|       0.0|         0.0|     null|                  0.3|       25.05|         2.0|      1.0|                2.75|\n",
      "|       1| 2020-01-01 01:19:57|  2020-01-01 01:30:56|                 N|       1.0|         210|         150|            1.0|          2.3|       10.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        11.3|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 01:52:33|  2020-01-01 02:09:54|                 N|       1.0|          35|          39|            1.0|          3.0|       13.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        14.8|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:10:18|  2020-01-01 01:22:16|                 N|       1.0|          25|          61|            1.0|         2.77|       11.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        12.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 02:03:14|  2020-01-01 02:29:45|                 N|       1.0|         225|          89|            1.0|         4.98|       20.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        21.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:04:11|  2020-01-01 01:09:48|                 N|       1.0|         129|         129|            1.0|         0.71|        5.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:25:52|  2020-01-01 01:32:16|                 N|       1.0|         129|          83|            1.0|          0.8|        5.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:47:32|  2020-01-01 01:59:25|                 N|       1.0|          82|         173|            1.0|         1.52|        9.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        10.8|         2.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 01:26:40|  2020-01-01 01:40:42|                 N|       1.0|          74|          69|            1.0|          3.8|       14.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:38:47|  2020-01-01 01:46:02|                 N|       1.0|          74|          41|            1.0|         1.12|        6.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.8|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:52:18|  2020-01-01 02:09:58|                 N|       1.0|          41|         127|            1.0|         5.67|       19.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        20.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:16:01|  2020-01-01 01:26:40|                 N|       1.0|           7|         260|            1.0|         1.86|        9.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        10.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:21:15|  2020-01-01 01:28:03|                 N|       1.0|           7|           7|            1.0|         1.42|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:35:42|  2020-01-01 02:02:00|                 N|       1.0|           7|         133|            1.0|        15.48|       43.0|  0.5|    0.5|      8.86|         0.0|     null|                  0.3|       53.16|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:39:51|  2020-01-01 01:42:54|                 N|       1.0|         134|          28|            1.0|         1.15|        5.5|  0.5|    0.5|       1.0|         0.0|     null|                  0.3|         7.8|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 01:00:21|  2020-01-01 01:10:19|                 N|       1.0|          89|          39|            1.0|          2.3|       10.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        11.3|         2.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 01:13:59|  2020-01-01 01:21:31|                 N|       1.0|          66|          65|            3.0|          1.0|        6.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.8|         2.0|      1.0|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WHERE` vs `filter()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the underlying RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "    .select(\"lpep_pickup_datetime\", \"PULocationID\", \"total_amount\") \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2019, 12, 18, 16, 52, 30), PULocationID=264, total_amount=4.81)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take 1st row of all records\n",
    "rdd.filter(lambda row: True).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take 1st row of no records\n",
    "rdd.filter(lambda row: False).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2021, month=1, day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start\n",
    "\n",
    "def prepare_for_grouping(row):\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "\n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)\n",
    "\n",
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "\n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "\n",
    "    return (output_amount, output_count)\n",
    "\n",
    "RevenueRow = namedtuple(\"RevenueRow\", [\"hour\", \"zone\", \"revenue\", \"count\"])\n",
    "\n",
    "# Un-Nest the rows to a single tuple per row\n",
    "def unwrap(row):\n",
    "    return RevenueRow(hour=row[0][0], \n",
    "                      zone=row[0][1], \n",
    "                      revenue=row[1][0], \n",
    "                      count=row[1][1])\n",
    "\n",
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True), \n",
    "    types.StructField('zone', types.IntegerType(), True), \n",
    "    types.StructField('revenue', types.DoubleType(), True), \n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/26 19:58:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet(\n",
    "    \"tmp/green-revenue\", \n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2021-10-01 03:00:00|  74|             31.96|    3|\n",
      "|2021-10-01 03:00:00|  82|              14.3|    1|\n",
      "|2021-10-01 03:00:00|  42|             45.91|    2|\n",
      "|2021-10-01 04:00:00|  95|               8.8|    1|\n",
      "|2021-10-01 05:00:00|  92|              30.3|    1|\n",
      "|2021-10-01 05:00:00| 244|             98.46|    2|\n",
      "|2021-10-01 06:00:00| 129|              70.1|    4|\n",
      "|2021-10-01 06:00:00|  41|              10.3|    1|\n",
      "|2021-10-01 07:00:00|   7|              39.6|    1|\n",
      "|2021-10-01 07:00:00| 159|             99.05|    1|\n",
      "|2021-10-01 07:00:00|  71|             97.85|    1|\n",
      "|2021-10-01 08:00:00| 200|121.19999999999999|    2|\n",
      "|2021-10-01 08:00:00|  24|               8.8|    1|\n",
      "|2021-10-01 08:00:00| 152|             28.55|    1|\n",
      "|2021-10-01 09:00:00| 146|               5.3|    1|\n",
      "|2021-10-01 09:00:00|  74|480.67000000000024|   36|\n",
      "|2021-10-01 09:00:00|  42|             264.2|   12|\n",
      "|2021-10-01 09:00:00| 130|             31.42|    2|\n",
      "|2021-10-01 09:00:00| 122|              11.3|    1|\n",
      "|2021-10-01 10:00:00| 179|             24.38|    1|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Spark RDD mapPartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example Appliction*: Using data from spark dataframe for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"VendorID\", \n",
    "    \"lpep_pickup_datetime\", \n",
    "    \"PULocationID\", \n",
    "    \"DOLocationID\", \n",
    "    \"trip_distance\"\n",
    "]\n",
    "\n",
    "duration_rdd = df_green.select(columns).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2019, 12, 18, 16, 52, 30), PULocationID=264, DOLocationID=264, trip_distance=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 1, 45, 58), PULocationID=66, DOLocationID=65, trip_distance=1.28),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 1, 41, 38), PULocationID=181, DOLocationID=228, trip_distance=2.47),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 1, 52, 46), PULocationID=129, DOLocationID=263, trip_distance=6.3),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 1, 19, 57), PULocationID=210, DOLocationID=150, trip_distance=2.3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ...\n",
    "\n",
    "def model_precit(df):\n",
    "    # y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_precit(df)\n",
    "    df[\"predicted_duration\"] = predictions\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|               0.0|\n",
      "|               6.4|\n",
      "|12.350000000000001|\n",
      "|              31.5|\n",
      "|              11.5|\n",
      "|              15.0|\n",
      "|             13.85|\n",
      "|24.900000000000002|\n",
      "|              3.55|\n",
      "|               4.0|\n",
      "|               7.6|\n",
      "|              19.0|\n",
      "|5.6000000000000005|\n",
      "|             28.35|\n",
      "|               9.3|\n",
      "|               7.1|\n",
      "|              77.4|\n",
      "|              5.75|\n",
      "|              11.5|\n",
      "|               5.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch) \\\n",
    "    .toDF() \\\n",
    "    .select(\"predicted_duration\") \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
