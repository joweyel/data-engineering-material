{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Homework \n",
    "\n",
    "In this homework we'll put what we learned about Spark in practice.\n",
    "\n",
    "For this homework we will be using the FHV 2019-10 data found here. [FHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-25 18:51:55--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240225%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240225T175146Z&X-Amz-Expires=300&X-Amz-Signature=c6b4d41786099ec1a0b687d76268889d07e9ffd415d575827ed2d905db7bcbee&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-25 18:51:55--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240225%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240225T175146Z&X-Amz-Expires=300&X-Amz-Signature=c6b4d41786099ec1a0b687d76268889d07e9ffd415d575827ed2d905db7bcbee&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "data/fhv_tripdata_2019-10.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p data\n",
    "FILE = \"fhv_tripdata_2019-10.csv.gz\"\n",
    "!wget -c https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/$FILE -O data/$FILE\n",
    "data_path = f\"data/{FILE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "\n",
    "**Install Spark and PySpark** \n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: \n",
    "\n",
    "- `spark.version`: 3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "**FHV October 2019**\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 1MB\n",
    "- 6MB\n",
    "- 25MB\n",
    "- 87MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.LongType(), True),\n",
    "    types.StructField('DOLocationID', types.LongType(), True),\n",
    "    types.StructField('SR_Flag', types.DoubleType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "])\n",
    "\n",
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv.repartition(6).write.parquet(\n",
    "    \"data/question2/\", \n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the file sizes of all generated parquet files of the repartitioned dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,4M\tdata/question2/part-00000-60cdf558-d507-497b-aa29-23b2cf604184-c000.snappy.parquet\n",
      "6,4M\tdata/question2/part-00001-60cdf558-d507-497b-aa29-23b2cf604184-c000.snappy.parquet\n",
      "6,4M\tdata/question2/part-00002-60cdf558-d507-497b-aa29-23b2cf604184-c000.snappy.parquet\n",
      "6,4M\tdata/question2/part-00003-60cdf558-d507-497b-aa29-23b2cf604184-c000.snappy.parquet\n",
      "6,4M\tdata/question2/part-00004-60cdf558-d507-497b-aa29-23b2cf604184-c000.snappy.parquet\n",
      "6,4M\tdata/question2/part-00005-60cdf558-d507-497b-aa29-23b2cf604184-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!du -sh data/question2/* | grep parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "- `6 MB`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "\n",
    "**Count records** \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "- 108,164\n",
    "- 12,856\n",
    "- 452,470\n",
    "- 62,610\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Be aware of columns order when defining schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.createOrReplaceTempView(\"trips_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|trips_15_oct|\n",
      "+------------+\n",
      "|       62610|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS trips_15_oct\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE\n",
    "    EXTRACT(day FROM pickup_datetime) = 15 AND\n",
    "    EXTRACT(month FROM pickup_datetime) = 10\n",
    "\"\"\")\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "- `62610`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: \n",
    "\n",
    "**Longest trip for each day** \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "- 631,152.50 Hours\n",
    "- 243.44 Hours\n",
    "- 7.68 Hours\n",
    "- 3.32 Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
